{
 "cells": [
  {
   "cell_type": "code",
   "id": "3d12a443",
   "metadata": {},
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ====================== 1. 读取数据 ======================\n",
    "with open('tokenized_poems.json', 'r', encoding='utf-8') as f:\n",
    "    poems = json.load(f)  # 直接得到分词后的二维列表\n",
    "\n",
    "# ====================== 2. 构建词表 ======================\n",
    "from collections import Counter\n",
    "\n",
    "# 得到所有token\n",
    "all_tokens_list = [token for poem in poems for para in poem['paragraphs'] for token in para]\n",
    "cnt = Counter(all_tokens_list)\n",
    "print(cnt.most_common(20))  # 查看常见词\n",
    "\n",
    "min_freq = 5  # 只保留出现>=10次的词，其他都归为<UNK>\n",
    "all_tokens = set([token for token in all_tokens_list if cnt[token] >= min_freq])\n",
    "\n",
    "tokens = ['<PAD>', '<START>', '<END>', '<UNK>'] + list(all_tokens)\n",
    "token2idx = {token: idx for idx, token in enumerate(tokens)}\n",
    "idx2token = {idx: token for token, idx in token2idx.items()}\n",
    "\n",
    "# 假设已有 token2idx 和 idx2token 字典\n",
    "with open('token2idx.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(token2idx, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open('idx2token.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(idx2token, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "vocab_size = len(tokens)\n",
    "print(f\"筛选后词汇表大小: {vocab_size}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a34b4bda",
   "metadata": {},
   "source": [
    "\n",
    "# ====================== 3. 数据预处理 ======================\n",
    "# 每个句子前加<START>，后加<END>\n",
    "max_len = max(len(para) for poem in poems for para in poem['paragraphs']) + 2  # +2 for <START> <END>\n",
    "\n",
    "def poem_to_ids(poem):\n",
    "    ids = [token2idx['<START>']]\n",
    "    for token in poem:\n",
    "        ids.append(token2idx.get(token, token2idx['<UNK>']))\n",
    "    ids.append(token2idx['<END>'])\n",
    "    # PAD到max_len\n",
    "    while len(ids) < max_len:\n",
    "        ids.append(token2idx['<PAD>'])\n",
    "    return ids\n",
    "\n",
    "data = [poem_to_ids(para) for poem in poems for para in poem['paragraphs']]\n",
    "\n",
    "# ====================== 4. 自定义数据集 ======================\n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][:-1]  # 输入序列\n",
    "        y = self.data[idx][1:]   # 目标序列（预测下一个词）\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "dataset = PoemDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2242a927",
   "metadata": {},
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ====================== 5. 定义 Transformer 模型 ======================\n",
    "class PoemTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=256, num_heads=8, num_layers=6, ff_dim=512, dropout=0.1):\n",
    "        super(PoemTransformer, self).__init__()\n",
    "        \n",
    "        # 词向量层\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=token2idx['<PAD>'])\n",
    "        \n",
    "        # 位置编码（Transformer不使用RNN，因此需要位置编码）\n",
    "        self.positional_encoding = nn.Parameter(torch.rand(1, 5000, embed_size))  # 最大序列长度 5000\n",
    "        \n",
    "        # Transformer 层\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # 输出层\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        # 将输入和目标嵌入并加上位置编码\n",
    "        src = self.embed(src) + self.positional_encoding[:, :src.size(1), :]\n",
    "        tgt = self.embed(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n",
    "        \n",
    "        # Transformer 期望的输入格式是 (sequence_length, batch_size, features)\n",
    "        src = src.transpose(0, 1)  # 转换成 (T, B, E)\n",
    "        tgt = tgt.transpose(0, 1)  # 转换成 (T, B, E)\n",
    "        \n",
    "        # 通过 Transformer 层\n",
    "        output = self.transformer(src, tgt)\n",
    "        \n",
    "        # 通过全连接层将输出映射回词汇表大小\n",
    "        output = self.fc_out(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# ====================== 6. 训练模型 ======================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 初始化模型，优化器和损失函数\n",
    "model = PoemTransformer(vocab_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # 使用较小的学习率\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token2idx['<PAD>'])\n",
    "\n",
    "# 训练轮次\n",
    "epochs = 10\n",
    "\n",
    "# 用于记录每个epoch的损失和困惑度\n",
    "epoch_losses = []\n",
    "epoch_perplexities = []\n",
    "\n",
    "# 初始化图表\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "loss_line, = ax1.plot([], [], label='Loss', color='blue')\n",
    "perplexity_line, = ax2.plot([], [], label='Perplexity', color='red')\n",
    "\n",
    "ax1.set_title('Training Loss over Epochs')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlim(0, epochs)\n",
    "ax1.set_ylim(0, 5)\n",
    "\n",
    "ax2.set_title('Training Perplexity over Epochs')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Perplexity')\n",
    "ax2.set_xlim(0, epochs)\n",
    "ax2.set_ylim(0, 50)\n",
    "\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "\n",
    "plt.ion()  # 开启交互模式\n",
    "\n",
    "# ====================== 训练过程 ======================\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_perplexity = 0\n",
    "    for x, y in tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}', ncols=100):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 将源序列（x）和目标序列（y）传入 Transformer 模型\n",
    "        output = model(x, y[:, :-1])  # 使用目标序列作为输入（右移 1 位）\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = criterion(output.view(-1, output_dim), y[:, 1:].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 计算困惑度\n",
    "        perplexity = torch.exp(loss)\n",
    "        total_perplexity += perplexity.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_perplexity = total_perplexity / len(dataloader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    epoch_perplexities.append(avg_perplexity)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}, Avg Perplexity: {avg_perplexity:.4f}')\n",
    "    \n",
    "    # 更新图表\n",
    "    loss_line.set_data(range(1, epoch + 2), epoch_losses)\n",
    "    perplexity_line.set_data(range(1, epoch + 2), epoch_perplexities)\n",
    "    \n",
    "    ax1.relim()\n",
    "    ax1.autoscale_view()\n",
    "    ax2.relim()\n",
    "    ax2.autoscale_view()\n",
    "    \n",
    "    plt.draw()\n",
    "    plt.pause(0.1)\n",
    "    \n",
    "    # 保存模型权重（每个 epoch 保存一次）\n",
    "    torch.save(model.state_dict(), f'poem_transformer_epoch_{epoch+1}.pth')\n",
    "\n",
    "# 训练结束后保存最终模型\n",
    "torch.save(model.state_dict(), 'poem_transformer_final.pth')\n",
    "\n",
    "plt.ioff()  # 关闭交互模式\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

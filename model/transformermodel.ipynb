{
 "cells": [
  {
   "cell_type": "code",
   "id": "817db68d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T07:26:20.325694Z",
     "start_time": "2025-07-07T07:26:18.428851Z"
    }
   },
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "# ====================== 1. 加载词表 ======================\n",
    "def load_vocab(file_path: str) -> Dict[str, int]:\n",
    "    \"\"\"加载词表映射文件\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# ====================== 2. 定义 Transformer 模型 ======================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"位置编码，为模型提供序列位置信息\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, embed_dim]\n",
    "        return x + self.pe[:x.size(1), :]\n",
    "\n",
    "class PoemTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=256, num_heads=8, num_layers=6, ff_dim=512, dropout=0.1):\n",
    "        super(PoemTransformer, self).__init__()\n",
    "        \n",
    "        # 词向量层\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)  # 临时设为0，主程序中修正\n",
    "        \n",
    "        # 位置编码\n",
    "        self.positional_encoding = nn.Parameter(torch.rand(1, 5000, embed_size))\n",
    "        \n",
    "        # 完整Transformer结构\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 输出层\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"生成自回归掩码\"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        # 嵌入和位置编码\n",
    "        src = self.dropout(self.embed(src) + self.positional_encoding[:, :src.size(1), :])\n",
    "        tgt = self.dropout(self.embed(tgt) + self.positional_encoding[:, :tgt.size(1), :])\n",
    "        \n",
    "        # 生成掩码\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        src_mask = self.generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
    "        \n",
    "        # 通过Transformer\n",
    "        output = self.transformer(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        \n",
    "        return self.fc_out(output), None\n",
    "\n",
    "# ====================== 3. 古诗生成器（含叠词优化） ======================\n",
    "class PoemGenerator:\n",
    "    def __init__(self,\n",
    "                 token2idx: Dict[str, int],\n",
    "                 idx2token: Dict[int, str],\n",
    "                 model: torch.nn.Module,\n",
    "                 device: str = \"cpu\"):\n",
    "\n",
    "        self.token2idx = token2idx\n",
    "        self.idx2token = idx2token\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.associations = self._load_associations(\"word_associations.json\")\n",
    "        \n",
    "        # 初始化分词器\n",
    "        try:\n",
    "            import thulac\n",
    "            self.thulac = thulac.thulac(seg_only=True)\n",
    "        except ImportError:\n",
    "            print(\"警告: 未安装THULAC分词器，将使用简单分词方法\")\n",
    "            self.thulac = None\n",
    "\n",
    "    def _load_associations(self, path: str) -> Dict:\n",
    "        \"\"\"加载关联词库\"\"\"\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        print(f\"警告: 未找到关联词库文件 {path}，将使用空字典。\")\n",
    "        return {}\n",
    "\n",
    "    def segment_with_thulac(self, text: str) -> List[str]:\n",
    "        \"\"\"分词方法\"\"\"\n",
    "        if self.thulac:\n",
    "            result = self.thulac.cut(text)\n",
    "            return [word for word, _ in result]\n",
    "        return list(text)  # 回退到单字分词\n",
    "\n",
    "    def get_associated_words(self, word: str, top_k: int = 5) -> List[str]:\n",
    "        \"\"\"获取输入词的关联词列表\"\"\"\n",
    "        if word not in self.associations:\n",
    "            return []\n",
    "        related_words = sorted(\n",
    "            self.associations[word].items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        return [w for w, _ in related_words[:top_k]]\n",
    "\n",
    "    def generate_poem(self, prompt: str, max_len: int = 32, temperature=0.6, max_repeat=2) -> str:\n",
    "        \"\"\"\n",
    "        生成古诗，含叠词抑制\n",
    "        max_repeat: 允许的最大连续相同字符数（默认2）\n",
    "        \"\"\"\n",
    "        # 1. 分词与联想\n",
    "        segmented_words = self.segment_with_thulac(prompt)\n",
    "        print(f\"分词结果: {segmented_words}\")\n",
    "        \n",
    "        # 收集候选词\n",
    "        all_candidates = []\n",
    "        for word in segmented_words:\n",
    "            all_candidates.append(word)\n",
    "            all_candidates.extend(self.get_associated_words(word, top_k=5))\n",
    "        \n",
    "        # 过滤有效词，补充主题相关常用字\n",
    "        valid_words = list(set(w for w in all_candidates if w in self.token2idx))\n",
    "        if not valid_words:\n",
    "            valid_words = [w for w in list(self.token2idx.keys())[:10] if w not in ['<PAD>', '<START>']]\n",
    "        \n",
    "        # 根据主题补充候选词（减少无关字符）\n",
    "        theme_related = {\n",
    "            '春天': ['春', '风', '花', '柳', '燕', '啼', '暖', '芽'],\n",
    "            '秋天': ['秋', '霜', '叶', '月', '雁', '寒', '枫', '露'],\n",
    "            '夏天': ['夏', '荷', '蝉', '雨', '荫', '热', '蛙', '莲'],\n",
    "            '冬天': ['冬', '雪', '寒', '梅', '冰', '霜', '风', '松']\n",
    "        }.get(prompt, ['日', '云', '山', '水', '天', '地', '人', '心'])\n",
    "        \n",
    "        valid_words.extend([w for w in theme_related if w in self.token2idx and w not in valid_words])\n",
    "        valid_words = list(set(valid_words))  # 去重\n",
    "        print(f\"有效候选词: {valid_words}\")\n",
    "\n",
    "        # 2. 初始化输入\n",
    "        input_ids = [self.token2idx[valid_words[0]]] if valid_words else [self.token2idx['<START>']]\n",
    "        generated = [self.idx2token[idx] for idx in input_ids]\n",
    "\n",
    "        # 3. 生成诗句（核心逻辑）\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            src = torch.tensor([input_ids]).to(self.device)\n",
    "            tgt = src.clone()\n",
    "            \n",
    "            for _ in range(max_len - len(input_ids)):\n",
    "                # 模型预测\n",
    "                output, _ = self.model(src, tgt)\n",
    "                next_probs = output[:, -1, :]  # 最后一个位置的预测\n",
    "                \n",
    "                # 处理重复字符：惩罚连续重复的字符\n",
    "                last_char = generated[-1] if generated else ''\n",
    "                repeat_count = 1\n",
    "                # 统计当前连续重复次数\n",
    "                for c in reversed(generated[:-1]):\n",
    "                    if c == last_char:\n",
    "                        repeat_count += 1\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                # 如果超过最大允许重复次数，降低重复字符的概率\n",
    "                if repeat_count >= max_repeat and last_char in self.token2idx:\n",
    "                    repeat_idx = self.token2idx[last_char]\n",
    "                    next_probs[0, repeat_idx] *= 0.1  # 惩罚重复字符\n",
    "                    next_probs = F.softmax(next_probs, dim=-1)  # 重新归一化\n",
    "                \n",
    "                # 温度采样\n",
    "                probs = F.softmax(next_probs / temperature, dim=-1)\n",
    "                next_idx = torch.multinomial(probs, 1).item()\n",
    "                next_word = self.idx2token.get(next_idx, '')\n",
    "                \n",
    "                # 过滤无效字符\n",
    "                invalid_tokens = ['<END>', '<UNK>', '<PAD>', '<START>']\n",
    "                if next_word in invalid_tokens:\n",
    "                    continue\n",
    "                \n",
    "                # 二次检查：避免新增重复\n",
    "                if generated and next_word == generated[-1] and repeat_count >= max_repeat:\n",
    "                    # 从候选词中选一个不同的字符\n",
    "                    alternatives = [w for w in valid_words if w != next_word and w not in invalid_tokens]\n",
    "                    if alternatives:\n",
    "                        next_word = random.choice(alternatives)\n",
    "                        next_idx = self.token2idx[next_word]\n",
    "                \n",
    "                # 添加到结果\n",
    "                generated.append(next_word)\n",
    "                input_ids.append(next_idx)\n",
    "                tgt = torch.tensor([input_ids]).to(self.device)\n",
    "                \n",
    "                # 满足长度则停止\n",
    "                if len(generated) >= 20:  # 4句x5字=20\n",
    "                    break\n",
    "\n",
    "        # 4. 格式化输出\n",
    "        poem = \"\".join(generated)\n",
    "        # 清理无效标记\n",
    "        for token in invalid_tokens:\n",
    "            poem = poem.replace(token, \"\")\n",
    "        \n",
    "        # 按5字一句分割\n",
    "        lines = []\n",
    "        for i in range(0, min(len(poem), 20), 5):\n",
    "            line = poem[i:i+5]\n",
    "            if len(line) == 5:\n",
    "                lines.append(line)\n",
    "        \n",
    "        # 不足4句则补充\n",
    "        if len(lines) < 4 and valid_words:\n",
    "            for _ in range(4 - len(lines)):\n",
    "                filler = \"\".join(random.sample(valid_words, min(5, len(valid_words))))\n",
    "                lines.append(filler[:5])\n",
    "        \n",
    "        return \"\\n\".join(lines[:4])\n",
    "\n",
    "# ====================== 4. 模型加载与主程序 ======================\n",
    "def load_model(vocab_size, model_path=None, device='cpu'):\n",
    "    \"\"\"加载模型并处理权重兼容问题\"\"\"\n",
    "    model = PoemTransformer(vocab_size).to(device)\n",
    "    if model_path and os.path.exists(model_path):\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n",
    "            print(f\"成功加载预训练模型: {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"加载模型失败: {e}\")\n",
    "            print(\"将使用随机初始化权重\")\n",
    "    else:\n",
    "        print(\"警告: 未找到预训练模型，将使用随机初始化权重\")\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置设备\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 加载词表\n",
    "    print(\"正在加载词表...\")\n",
    "    token2idx = load_vocab('token2idx.json')\n",
    "    idx2token = {int(k): v for k, v in load_vocab('idx2token.json').items()}\n",
    "    vocab_size = len(token2idx)\n",
    "    print(f\"词表加载完成，大小: {vocab_size}\")\n",
    "    \n",
    "    # 加载模型并修正padding_idx\n",
    "    print(\"正在加载模型...\")\n",
    "    model = load_model(vocab_size, 'poem_transformer.pth', device)\n",
    "    model.embed.padding_idx = token2idx.get('<PAD>', 0)  # 动态修正padding_idx\n",
    "    print(\"模型加载完成\")\n",
    "    \n",
    "    # 初始化生成器\n",
    "    print(\"正在初始化生成器...\")\n",
    "    generator = PoemGenerator(\n",
    "        token2idx=token2idx,\n",
    "        idx2token=idx2token,\n",
    "        model=model,\n",
    "        device=device\n",
    "    )\n",
    "    print(\"生成器初始化完成\")\n",
    "    \n",
    "    # 生成古诗（可修改prompt为\"春天\"/\"夏天\"/\"冬天\"）\n",
    "    print(\"\\n=== 开始生成古诗 ===\")\n",
    "    poem = generator.generate_poem(\"春天\", max_len=40, max_repeat=2)  # max_repeat控制连续重复\n",
    "    print(\"生成结果：\")\n",
    "    print(poem)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载词表...\n",
      "词表加载完成，大小: 105569\n",
      "正在加载模型...\n",
      "成功加载预训练模型: poem_transformer.pth\n",
      "模型加载完成\n",
      "正在初始化生成器...\n",
      "警告: 未找到关联词库文件 word_associations.json，将使用空字典。\n",
      "Model loaded succeed\n",
      "生成器初始化完成\n",
      "\n",
      "=== 开始生成古诗 ===\n",
      "分词结果: ['春天']\n",
      "有效候选词: ['风', '春', '柳', '燕', '暖', '花', '芽', '春天', '啼']\n",
      "生成结果：\n",
      "风辞聘风潮\n",
      "花燕暖芽啼\n",
      "风花春暖燕\n",
      "风春春天花\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "62163bbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T07:25:48.731462Z",
     "start_time": "2025-07-07T07:25:48.600318Z"
    }
   },
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import jieba\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "class EnhancedPoemGenerator:\n",
    "    def __init__(self, token2idx: Dict[str, int], idx2token: Dict[int, str], device='cpu'):\n",
    "        self.token2idx = token2idx\n",
    "        self.idx2token = idx2token\n",
    "        self.device = device\n",
    "        \n",
    "        # 增强的语言资源\n",
    "        self.rhyme_groups = self._init_enhanced_rhyme_groups()\n",
    "        self.pingze_map = self._init_enhanced_pingze_map()\n",
    "        self.thesaurus = self._init_enhanced_thesaurus()\n",
    "        self.theme_vectors = self._init_theme_vectors()\n",
    "        \n",
    "        # 格律模板\n",
    "        self.meter_templates = {\n",
    "            '五言绝句': {\n",
    "                'patterns': [\n",
    "                    ['仄', '仄', '平', '平', '仄'],\n",
    "                    ['平', '平', '仄', '仄', '平'],\n",
    "                    ['平', '平', '平', '仄', '仄'],\n",
    "                    ['仄', '仄', '仄', '平', '平']\n",
    "                ],\n",
    "                'rhyme_pos': [1, 3]  # 第二、四句押韵\n",
    "            },\n",
    "            '七言绝句': {\n",
    "                'patterns': [\n",
    "                    ['平', '平', '仄', '仄', '平', '平', '仄'],\n",
    "                    ['仄', '仄', '平', '平', '仄', '仄', '平'],\n",
    "                    ['仄', '仄', '平', '平', '平', '仄', '仄'],\n",
    "                    ['平', '平', '仄', '仄', '仄', '平', '平']\n",
    "                ],\n",
    "                'rhyme_pos': [1, 3]\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _init_enhanced_rhyme_groups(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"初始化押韵分组\"\"\"\n",
    "        return {\n",
    "            'a': ['花', '家', '华', '霞', '涯', '沙', '茶', '麻', '纱'],\n",
    "            'o': ['歌', '多', '河', '波', '罗', '梭', '柯', '戈', '磨'],\n",
    "            'e': ['车', '斜', '嗟', '些', '爷', '椰', '茄', '靴', '蝎'],\n",
    "            'i': ['枝', '时', '丝', '迟', '诗', '知', '痴', '池', '脂'],\n",
    "            'u': ['无', '图', '湖', '孤', '壶', '途', '酥', '糊', '乌'],\n",
    "            'ong': ['空', '红', '风', '中', '同', '东', '通', '工', '蓬'],\n",
    "            'an': ['山', '间', '闲', '还', '颜', '关', '湾', '环', '班'],\n",
    "            'ang': ['长', '香', '光', '阳', '堂', '芳', '昌', '央', '刚'],\n",
    "            'ai': ['来', '开', '台', '才', '哉', '埃', '该', '孩', '灾'],\n",
    "            'ei': ['飞', '归', '辉', '威', '非', '妃', '肥', '扉', '菲']\n",
    "        }\n",
    "\n",
    "    def _init_enhanced_pingze_map(self) -> Dict[str, str]:\n",
    "        \"\"\"初始化平仄映射\"\"\"\n",
    "        pingze = defaultdict(lambda: '仄')\n",
    "        \n",
    "        # 第一声（阴平）\n",
    "        pingze.update({\n",
    "            '春':'平', '风':'平', '秋':'平', '天':'平', '空':'平',\n",
    "            '山':'平', '花':'平', '飞':'平', '开':'平', '江':'平',\n",
    "            '烟':'平', '芳':'平', '清':'平', '新':'平', '声':'平'\n",
    "        })\n",
    "        \n",
    "        # 第二声（阳平）\n",
    "        pingze.update({\n",
    "            '年':'平', '来':'平', '时':'平', '人':'平', '明':'平',\n",
    "            '长':'平', '流':'平', '晴':'平', '头':'平', '寒':'平',\n",
    "            '林':'平', '门':'平', '前':'平', '行':'平', '情':'平'\n",
    "        })\n",
    "        \n",
    "        # 特殊入声字处理\n",
    "        pingze.update({\n",
    "            '月':'仄', '日':'仄', '雪':'仄', '白':'仄', '竹':'仄',\n",
    "            '石':'仄', '玉':'仄', '色':'仄', '绿':'仄', '落':'仄'\n",
    "        })\n",
    "        \n",
    "        return pingze\n",
    "\n",
    "    def _init_enhanced_thesaurus(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"初始化同类词库\"\"\"\n",
    "        return {\n",
    "            '季节': ['春', '夏', '秋', '冬', '晨', '夕', '朝', '夜'],\n",
    "            '天气': ['风', '雨', '雪', '霜', '露', '雾', '雷', '电'],\n",
    "            '植物': ['花', '草', '树', '柳', '梅', '兰', '竹', '菊'],\n",
    "            '动物': ['鸟', '燕', '莺', '雁', '鹤', '鹊', '鸥', '鸠'],\n",
    "            '山水': ['山', '水', '江', '河', '湖', '海', '溪', '峰'],\n",
    "            '建筑': ['楼', '台', '亭', '阁', '桥', '寺', '塔', '轩'],\n",
    "            '情感': ['愁', '思', '忆', '念', '忧', '喜', '悲', '欢']\n",
    "        }\n",
    "\n",
    "    def _init_theme_vectors(self) -> Dict[str, List[float]]:\n",
    "        \"\"\"初始化主题词向量\"\"\"\n",
    "        return {\n",
    "            '春': [0.9, 0.1, 0.3], '夏': [0.8, 0.2, 0.4],\n",
    "            '秋': [0.7, 0.3, 0.5], '冬': [0.6, 0.4, 0.6],\n",
    "            '风': [0.1, 0.9, 0.2], '花': [0.3, 0.8, 0.1],\n",
    "            '月': [0.2, 0.3, 0.8], '日': [0.8, 0.3, 0.2],\n",
    "            '山': [0.7, 0.1, 0.4], '水': [0.3, 0.6, 0.1],\n",
    "            '情': [0.1, 0.4, 0.9], '志': [0.4, 0.1, 0.9]\n",
    "        }\n",
    "\n",
    "    def generate_poem(self, model, theme_word: str, poem_type: str = '五言绝句', \n",
    "                     max_len: int = 32, strict: bool = True, temperature: float = 1.0) -> str:\n",
    "        \"\"\"\n",
    "        增强的主题约束古诗生成（优化为API接口）\n",
    "        :param model: 训练好的模型\n",
    "        :param theme_word: 主题词\n",
    "        :param poem_type: 诗体类型\n",
    "        :param max_len: 最大长度\n",
    "        :param strict: 是否严格模式\n",
    "        :param temperature: 生成温度\n",
    "        :return: 生成的诗（格式化字符串）\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        # 分词处理\n",
    "        try:\n",
    "            words = list(jieba.cut(theme_word))\n",
    "        except Exception:\n",
    "            words = list(theme_word)\n",
    "        \n",
    "        # 扩展主题相关词汇\n",
    "        theme_words = []\n",
    "        for word in words:\n",
    "            if word in self.token2idx:\n",
    "                theme_words.extend(self._expand_theme(word))\n",
    "            else:\n",
    "                theme_words.append(self._get_synonym_for_unknown_word(word))\n",
    "\n",
    "        theme_set = set(theme_words)\n",
    "        \n",
    "        # 初始化生成\n",
    "        start_token = self.token2idx.get(theme_word, self.token2idx['<START>'])\n",
    "        input_idx = torch.tensor([start_token]).unsqueeze(0).to(self.device)\n",
    "        generated = [self.idx2token.get(start_token, theme_word)]\n",
    "        \n",
    "        # 约束生成过程\n",
    "        for step in range(max_len - 1):\n",
    "            with torch.no_grad():\n",
    "                output, _ = model(input_idx, None)\n",
    "            \n",
    "            # 应用温度调节\n",
    "            logits = output[0, -1] / temperature\n",
    "            \n",
    "            # 主题约束采样\n",
    "            top_k = min(50, len(self.token2idx))\n",
    "            top_indices = torch.topk(logits, top_k).indices.tolist()\n",
    "            \n",
    "            # 优先选择主题相关词\n",
    "            valid_indices = [\n",
    "                i for i in top_indices \n",
    "                if self.idx2token[i] in theme_set or \n",
    "                self._is_theme_related(self.idx2token[i], theme_words)\n",
    "            ]\n",
    "            \n",
    "            # 回退机制\n",
    "            if not valid_indices:\n",
    "                valid_indices = top_indices[:10]\n",
    "            \n",
    "            # 采样下一个token\n",
    "            probs = torch.softmax(logits[valid_indices], -1)\n",
    "            next_idx = torch.multinomial(probs, 1).item()\n",
    "            next_token_idx = valid_indices[next_idx]\n",
    "            \n",
    "            # 结束条件\n",
    "            if next_token_idx == self.token2idx.get('<END>', -1):\n",
    "                break\n",
    "                \n",
    "            generated.append(self.idx2token[next_token_idx])\n",
    "            input_idx = torch.tensor([next_token_idx]).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # 规范化处理并返回格式化结果\n",
    "        return self.normalize_poem(''.join(generated[1:]), poem_type, strict, theme_word)\n",
    "\n",
    "    def normalize_poem(self, raw_text: str, poem_type: str, strict: bool, theme_word: str) -> str:\n",
    "        \"\"\"规范化生成的诗句，返回格式化文本\"\"\"\n",
    "        lines = self._basic_formatting(raw_text, poem_type)\n",
    "        \n",
    "        if theme_word:\n",
    "            lines = self._enhance_theme(lines, theme_word)\n",
    "        \n",
    "        if strict:\n",
    "            lines = self._strict_normalization(lines, poem_type)\n",
    "        \n",
    "        title = self._generate_title(lines, theme_word)\n",
    "        \n",
    "        # 格式化输出（添加空格和换行）\n",
    "        formatted_lines = []\n",
    "        for line in lines:\n",
    "            line = line.replace('，', '').replace('。', '')\n",
    "            spaced_line = ' '.join(list(line))\n",
    "            formatted_lines.append(spaced_line)\n",
    "        \n",
    "        return f\"{title}\\n\\n\" + \"\\n\".join(formatted_lines)\n",
    "\n",
    "    # 辅助方法（保持原有逻辑不变）\n",
    "    def _basic_formatting(self, text: str, poem_type: str) -> List[str]:\n",
    "        char_per_line = 5 if '五言' in poem_type else 7\n",
    "        cleaned = re.sub(r'[^\\u4e00-\\u9fa5]', '', text)\n",
    "        \n",
    "        lines = []\n",
    "        current_line = []\n",
    "        \n",
    "        for char in cleaned:\n",
    "            if len(current_line) < char_per_line:\n",
    "                current_line.append(char)\n",
    "            else:\n",
    "                punctuation = '，' if len(lines) % 2 == 0 else '。'\n",
    "                lines.append(''.join(current_line) + punctuation)\n",
    "                current_line = [char]\n",
    "        \n",
    "        if current_line:\n",
    "            lines.append(''.join(current_line) + ('。' if len(lines) % 2 == 1 else '。'))\n",
    "        \n",
    "        while len(lines) < 4:\n",
    "            placeholder = random.choice(['望', '观', '见', '看']) + '〇' * (char_per_line - 1)\n",
    "            lines.append(placeholder + '。')\n",
    "        \n",
    "        return lines[:4]\n",
    "\n",
    "    def _enhance_theme(self, lines: List[str], theme_word: str) -> List[str]:\n",
    "        if not any(theme_word in line for line in lines):\n",
    "            replace_pos = random.choice([0, len(lines)-1])\n",
    "            line = lines[replace_pos]\n",
    "            if len(line) >= 2:\n",
    "                pos = random.randint(0, len(line)-2)\n",
    "                lines[replace_pos] = line[:pos] + theme_word + line[pos+1:]\n",
    "        \n",
    "        season_words = [w for w in self.thesaurus['季节'] if any(w in line for line in lines)]\n",
    "        if season_words:\n",
    "            primary_season = season_words[0]\n",
    "            for i, line in enumerate(lines):\n",
    "                for season in self.thesaurus['季节']:\n",
    "                    if season != primary_season and season in line:\n",
    "                        lines[i] = line.replace(season, primary_season)\n",
    "        \n",
    "        return lines\n",
    "\n",
    "    def _strict_normalization(self, lines: List[str], poem_type: str) -> List[str]:\n",
    "        template = self.meter_templates.get(poem_type, {})\n",
    "        patterns = template.get('patterns', [])\n",
    "        rhyme_pos = template.get('rhyme_pos', [])\n",
    "        \n",
    "        if len(lines) >= 2 and rhyme_pos:\n",
    "            rhyme_group = None\n",
    "            for pos in rhyme_pos:\n",
    "                if pos < len(lines):\n",
    "                    line = lines[pos]\n",
    "                    rhyme_char = line[-2] if line[-1] == '。' else line[-1]\n",
    "                    rhyme_group = self._find_rhyme_group(rhyme_char) or rhyme_group\n",
    "\t\t\t\n",
    "            if rhyme_group:\n",
    "                for pos in rhyme_pos:\n",
    "                    if pos < len(lines):\n",
    "                        line = lines[pos]\n",
    "                        last_char = line[-2] if line[-1] == '。' else line[-1]\n",
    "                        if self._find_rhyme_group(last_char) != rhyme_group:\n",
    "                            candidates = self._get_rhyme_candidates(rhyme_group)\n",
    "                            if candidates:\n",
    "                                new_char = random.choice(candidates)\n",
    "                                lines[pos] = line[:-1] + new_char + line[-1]\n",
    "\t\t\n",
    "        if len(lines) == len(patterns):\n",
    "            for i in range(len(lines)):\n",
    "                line = lines[i]\n",
    "                pattern = patterns[i]\n",
    "                \n",
    "                new_line = []\n",
    "                for j in range(min(len(line)-1, len(pattern))):\n",
    "                    char = line[j]\n",
    "                    expected = pattern[j]\n",
    "                    \n",
    "                    if self.pingze_map[char] != expected:\n",
    "                        synonyms = self._find_synonyms(char)\n",
    "                        for syn in synonyms:\n",
    "                            if self.pingze_map[syn] == expected:\n",
    "                                char = syn\n",
    "                                break\n",
    "                    \n",
    "                    new_line.append(char)\n",
    "                \n",
    "                new_line.append(line[-1])\n",
    "                lines[i] = ''.join(new_line)\n",
    "\t\t\n",
    "        common_chars = set(self.pingze_map.keys()).union(set(self.thesaurus.keys()))\n",
    "        for i, line in enumerate(lines):\n",
    "            new_line = []\n",
    "            for char in line:\n",
    "                if char not in common_chars and char not in ['，', '。', '〇']:\n",
    "                    synonyms = self._find_synonyms(char)\n",
    "                    char = synonyms[0] if synonyms else '〇'\n",
    "                new_line.append(char)\n",
    "            lines[i] = ''.join(new_line)\n",
    "        \n",
    "        return lines\n",
    "\n",
    "    def _generate_title(self, lines: List[str], theme_word: str) -> str:\n",
    "        if theme_word:\n",
    "            suffixes = ['吟', '颂', '赋', '词', '曲', '谣', '诗', '歌', '叹', '篇', '章']\n",
    "            return theme_word + random.choice(suffixes)\n",
    "        \n",
    "        first_line = lines[0].strip('，。')\n",
    "        if len(first_line) >= 2:\n",
    "            suffixes = ['即景', '有感', '杂咏', '偶成', '抒怀', '寄情', '漫兴', '遣怀']\n",
    "            return first_line[:2] + random.choice(suffixes)\n",
    "        return first_line[0] + '吟'\n",
    "\n",
    "    def _find_rhyme_group(self, char: str) -> str:\n",
    "        for group, chars in self.rhyme_groups.items():\n",
    "            if char in chars:\n",
    "                return group\n",
    "        return ''\n",
    "\n",
    "    def _get_rhyme_candidates(self, group: str) -> List[str]:\n",
    "        return self.rhyme_groups.get(group, [])\n",
    "\n",
    "    def _find_synonyms(self, char: str) -> List[str]:\n",
    "        for category, words in self.thesaurus.items():\n",
    "            if char in words:\n",
    "                return words\n",
    "        return [char]\n",
    "\n",
    "    def _expand_theme(self, word: str) -> List[str]:\n",
    "        \"\"\"扩展主题相关词汇\"\"\"\n",
    "        expanded = [word]\n",
    "        for category, words in self.thesaurus.items():\n",
    "            if word in words:\n",
    "                expanded.extend(words)\n",
    "                break\n",
    "        return expanded\n",
    "\n",
    "    def _is_theme_related(self, word: str, theme_words: List[str]) -> bool:\n",
    "        \"\"\"判断词语是否与主题相关\"\"\"\n",
    "        if word in theme_words:\n",
    "            return True\n",
    "        \n",
    "        # 检查是否在同一主题类别中\n",
    "        for category, words in self.thesaurus.items():\n",
    "            if word in words and any(t in words for t in theme_words):\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def _get_synonym_for_unknown_word(self, word: str) -> str:\n",
    "        \"\"\"为未知词获取同义词\"\"\"\n",
    "        for category, words in self.thesaurus.items():\n",
    "            if word in words:\n",
    "                return random.choice(words)\n",
    "        return word\n",
    "    \n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "ab26411f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T07:25:49.580070Z",
     "start_time": "2025-07-07T07:25:48.779472Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# ====================== Transformer模型 ======================\n",
    "class PoemTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=128, num_heads=4, num_layers=2, ff_dim=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # 词嵌入层，padding_idx使用词表中的<PAD>索引（与前面代码保持一致）\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=2)  # 前面代码中<PAD>固定为2\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, 100, embed_size))\n",
    "        \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def generate_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        return mask.float().masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embed(src) + self.pos_encoder[:, :src.size(1)]\n",
    "        tgt = self.embed(tgt) + self.pos_encoder[:, :tgt.size(1)]\n",
    "        \n",
    "        tgt_mask = self.generate_mask(tgt.size(1)).to(tgt.device)\n",
    "        output = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        return self.fc_out(output), None\n",
    "\n",
    "# ====================== 古诗生成器 ======================\n",
    "class PoemGenerator:\n",
    "    def __init__(self, token2idx, idx2token, rhyme_dict=None, theme_words=None, device='cpu'):\n",
    "        # 直接使用前面代码的词表映射（核心：保持与token2idx.json/idx2token.json一致）\n",
    "        self.token2idx = token2idx  # 来自token2idx.json\n",
    "        self.idx2token = idx2token  # 来自idx2token.json\n",
    "        self.device = device\n",
    "        \n",
    "        # 押韵字典和主题词库（可选，不影响词表统一性）\n",
    "        self.rhyme_dict = rhyme_dict or self._build_default_rhyme_dict()\n",
    "        self.theme_words = theme_words or self._build_default_theme_words()\n",
    "        \n",
    "        # 通用词库严格从词表中提取（确保只用词表中的字）\n",
    "        self.common_words = [char for char in token2idx \n",
    "                            if char not in ['<START>', '<END>', '<PAD>']]  # 排除特殊标记\n",
    "\n",
    "    def _build_default_rhyme_dict(self) -> Dict[str, str]:\n",
    "        rhyme_dict = defaultdict(str)\n",
    "        categories = {\n",
    "            'a': ['花', '家', '霞', '沙', '茶', '麻', '涯', '瓜', '华'],\n",
    "            'o': ['歌', '多', '河', '波', '罗', '梭', '柯', '戈', '磨'],\n",
    "            'i': ['衣', '期', '池', '知', '时', '丝', '诗', '棋', '词'],\n",
    "            'u': ['无', '图', '湖', '孤', '壶', '途', '苏', '书', '珠'],\n",
    "            'an': ['山', '间', '闲', '还', '颜', '关', '湾', '环', '班'],\n",
    "            'ang': ['长', '香', '光', '阳', '堂', '芳', '昌', '央', '刚'],\n",
    "            'eng': ['风', '声', '灯', '僧', '升', '生', '星', '耕', '更'],\n",
    "            'ong': ['东', '同', '风', '中', '空', '红', '通', '工', '蓬']\n",
    "        }\n",
    "        for rhyme, chars in categories.items():\n",
    "            for char in chars:\n",
    "                rhyme_dict[char] = rhyme\n",
    "        return rhyme_dict\n",
    "\n",
    "    def _build_default_theme_words(self) -> Dict[str, List[str]]:\n",
    "        return {\n",
    "            '春': ['春', '花', '柳', '燕', '莺', '风', '雨', '暖', '芽', '绿'],\n",
    "            '夏': ['夏', '荷', '蝉', '蛙', '阳', '热', '雨', '莲', '塘', '炎'],\n",
    "            '秋': ['秋', '月', '菊', '雁', '霜', '叶', '凉', '风', '黄', '收'],\n",
    "            '冬': ['冬', '雪', '梅', '冰', '寒', '风', '凌', '松', '白', '冷'],\n",
    "            '月': ['月', '夜', '明', '光', '影', '秋', '江', '湖', '镜', '望'],\n",
    "            '山': ['山', '峰', '岭', '石', '云', '雾', '高', '青', '翠', '险'],\n",
    "            '水': ['水', '河', '湖', '海', '波', '浪', '流', '清', '碧', '深']\n",
    "        }\n",
    "\n",
    "    def generate_poem(self, model, theme_word: str, poem_type: str = '五言绝句', \n",
    "                     max_len: int = 32, strict: bool = True, temperature: float = 1.0) -> str:\n",
    "        model.eval()\n",
    "        \n",
    "        # 每句字数（五言/七言）\n",
    "        chars_per_line = 5 if '五言' in poem_type else 7\n",
    "        lines_count = 4\n",
    "        \n",
    "        # 主题词严格限制在词表内\n",
    "        if theme_word not in self.token2idx:\n",
    "            # 如果主题词不在词表，从词表中找近似词（确保不超出词表范围）\n",
    "            theme_word = random.choice(self.common_words[:5])\n",
    "        \n",
    "        # 主题相关词仅从词表中筛选\n",
    "        theme_related = self.theme_words.get(theme_word, [theme_word])\n",
    "        theme_related = [w for w in theme_related if w in self.token2idx]  # 过滤词表外的字\n",
    "        theme_related.extend(self.common_words)\n",
    "        theme_set = set(theme_related)\n",
    "        \n",
    "        # 初始化生成（严格使用词表中的索引）\n",
    "        start_token = self.token2idx[theme_word]  # 直接从词表取索引\n",
    "        input_idx = torch.tensor([[start_token]]).to(self.device)\n",
    "        generated = [self.idx2token[start_token]]  # 从词表取对应字\n",
    "        \n",
    "        # 押韵控制（仅用词表内的字）\n",
    "        rhyme_char = None\n",
    "        if strict:\n",
    "            rhyme_pool = [char for char in self.rhyme_dict if char in self.token2idx]\n",
    "            if rhyme_pool:\n",
    "                rhyme_char = random.choice(rhyme_pool)\n",
    "                rhyme_category = self.rhyme_dict[rhyme_char]\n",
    "        \n",
    "        # 生成逻辑（全程限制在词表内）\n",
    "        current_line = 0\n",
    "        current_char = 0\n",
    "        \n",
    "        while len(generated) < max_len and current_line < lines_count:\n",
    "            with torch.no_grad():\n",
    "                output, _ = model(input_idx, input_idx)\n",
    "            \n",
    "            # 温度调节\n",
    "            logits = output[0, -1] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # 候选词严格来自词表\n",
    "            top_k = 50\n",
    "            top_indices = torch.topk(probs, top_k).indices.tolist()\n",
    "            valid_indices = [i for i in top_indices if self.idx2token[i] in theme_set]\n",
    "            \n",
    "            # 押韵字也必须在词表内\n",
    "            if strict and rhyme_char and (current_line == 1 or current_line == 3) and current_char == chars_per_line - 1:\n",
    "                rhyme_candidates = [i for i in valid_indices if self.rhyme_dict.get(self.idx2token[i]) == rhyme_category]\n",
    "                if rhyme_candidates:\n",
    "                    valid_indices = rhyme_candidates\n",
    "            \n",
    "            if not valid_indices:\n",
    "                valid_indices = top_indices[:10]\n",
    "            \n",
    "            # 采样（确保结果在词表内）\n",
    "            probs = F.softmax(logits[valid_indices], dim=-1)\n",
    "            next_idx = torch.multinomial(probs, 1).item()\n",
    "            next_token_idx = valid_indices[next_idx]\n",
    "            next_token = self.idx2token[next_token_idx]  # 从词表取字\n",
    "            \n",
    "            if next_token in ['<END>', '<PAD>']:\n",
    "                break\n",
    "                \n",
    "            generated.append(next_token)\n",
    "            input_idx = torch.tensor([[next_token_idx]]).to(self.device)\n",
    "            \n",
    "            # 更新计数\n",
    "            current_char += 1\n",
    "            if current_char >= chars_per_line:\n",
    "                current_char = 0\n",
    "                current_line += 1\n",
    "        \n",
    "        # 格式化输出（仅包含词表内的字）\n",
    "        return self.format_poem(generated, chars_per_line, lines_count)\n",
    "    \n",
    "    def format_poem(self, tokens: List[str], chars_per_line: int, lines_count: int) -> str:\n",
    "        poem = []\n",
    "        current_line = \"\"\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in ['<START>', '<END>', '<PAD>']:\n",
    "                continue\n",
    "            current_line += token\n",
    "            if len(current_line) == chars_per_line:\n",
    "                poem.append(current_line)\n",
    "                current_line = \"\"\n",
    "            if len(poem) >= lines_count:\n",
    "                break\n",
    "        \n",
    "        # 不足时用词表中的字补充\n",
    "        while len(poem) < lines_count:\n",
    "            filler = \"\".join(random.sample(self.common_words, min(chars_per_line, len(self.common_words))))\n",
    "            poem.append(filler[:chars_per_line])\n",
    "        \n",
    "        return \"\\n\".join(poem)\n",
    "\n",
    "# ====================== 词表加载（与前面代码完全一致） ======================\n",
    "def load_vocab(vocab_path: str) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"完全复用前面代码的词表加载逻辑，确保使用同一个token2idx.json和idx2token.json\"\"\"\n",
    "    try:\n",
    "        with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "            token2idx = json.load(f)  # 加载前面代码的token2idx.json\n",
    "        idx2token = {v: k for k, v in token2idx.items()}  # 保持与前面代码一致的反向映射\n",
    "        print(f\"成功加载词表（与前面代码共用），大小: {len(token2idx)}\")\n",
    "        return token2idx, idx2token\n",
    "    except FileNotFoundError:\n",
    "        print(f\"未找到词表文件，自动生成默认词表（与前面代码格式一致）\")\n",
    "        # 自动生成时，格式与前面代码完全相同（特殊标记索引固定）\n",
    "        default_chars = \"春夏秋冬风雨雪花草树木山石水云日月星天地人\"\n",
    "        token2idx = {\n",
    "            \"<START>\": 0,  # 与前面代码一致\n",
    "            \"<END>\": 1,    # 与前面代码一致\n",
    "            \"<PAD>\": 2     # 与前面代码一致\n",
    "        }\n",
    "        for i, char in enumerate(default_chars, 3):\n",
    "            token2idx[char] = i\n",
    "        idx2token = {v: k for k, v in token2idx.items()}\n",
    "        # 自动保存为与前面代码相同的文件，确保后续复用\n",
    "        with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(token2idx, f, ensure_ascii=False)\n",
    "        return token2idx, idx2token\n",
    "\n",
    "def load_rhyme_dict(rhyme_path: str) -> Dict[str, str]:\n",
    "    try:\n",
    "        with open(rhyme_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"未找到押韵字典，使用默认值\")\n",
    "        return {}\n",
    "\n",
    "def load_theme_words(theme_path: str) -> Dict[str, List[str]]:\n",
    "    try:\n",
    "        with open(theme_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"未找到主题词库，使用默认值\")\n",
    "        return {}\n",
    "\n",
    "# ====================== 主函数（确保词表统一） ======================\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"使用设备: {device}\")\n",
    "    \n",
    "    \n",
    "    token2idx, idx2token = load_vocab('token2idx.json')  \n",
    "    \n",
    "    # 加载其他辅助文件（不影响词表统一性）\n",
    "    rhyme_dict = load_rhyme_dict(\"rhyme_dict.json\")\n",
    "    theme_words = load_theme_words(\"theme_words.json\")\n",
    "    \n",
    "    # 初始化模型（基于统一的词表大小）\n",
    "    vocab_size = len(token2idx)\n",
    "    model = PoemTransformer(vocab_size).to(device)\n",
    "    \n",
    "    # 初始化生成器（严格传入统一的词表）\n",
    "    generator = PoemGenerator(\n",
    "        token2idx=token2idx,\n",
    "        idx2token=idx2token,\n",
    "        rhyme_dict=rhyme_dict,\n",
    "        theme_words=theme_words,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # 测试生成（结果完全基于统一词表）\n",
    "    print(\"\\n===== 生成测试（基于统一词表） =====\")\n",
    "    test_cases = [\n",
    "        {\"theme\": \"春\", \"type\": \"五言绝句\", \"temp\": 0.8},\n",
    "        {\"theme\": \"月\", \"type\": \"七言绝句\", \"temp\": 0.9},\n",
    "        {\"theme\": \"山\", \"type\": \"五言绝句\", \"temp\": 1.0},\n",
    "        {\"theme\": \"水\", \"type\": \"七言绝句\", \"temp\": 1.0}\n",
    "    ]\n",
    "    \n",
    "    for case in test_cases:\n",
    "        print(f\"\\n----- 主题: {case['theme']} | 诗体: {case['type']} -----\")\n",
    "        poem = generator.generate_poem(\n",
    "            model=model,\n",
    "            theme_word=case['theme'],\n",
    "            poem_type=case['type'],\n",
    "            temperature=case['temp']\n",
    "        )\n",
    "        print(poem)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n",
      "成功加载词表（与前面代码共用），大小: 105569\n",
      "未找到押韵字典，使用默认值\n",
      "未找到主题词库，使用默认值\n",
      "\n",
      "===== 生成测试（基于统一词表） =====\n",
      "\n",
      "----- 主题: 春 | 诗体: 五言绝句 -----\n",
      "春望夫双羽\n",
      "庶买得逆耳\n",
      "宁独冷凝觌\n",
      "荻花风喷雪\n",
      "\n",
      "----- 主题: 月 | 诗体: 七言绝句 -----\n",
      "谬以湘江祖帐薪\n",
      "不动携策种豆伴\n",
      "不得闲吠二十篇\n",
      "昔初选调剥皮蒙\n",
      "\n",
      "----- 主题: 山 | 诗体: 五言绝句 -----\n",
      "气含星分深\n",
      "小龙咽津莫\n",
      "外貌囘麦秋\n",
      "默有浓于当\n",
      "\n",
      "----- 主题: 水 | 诗体: 七言绝句 -----\n",
      "侐能荐真细事功\n",
      "野烧稠叠客于怀\n",
      "怀今公清日下唯\n",
      "飞霙逐境听逢逢\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "6d33cfe1",
   "metadata": {},
   "source": [
    "####温度+押韵"
   ]
  },
  {
   "cell_type": "code",
   "id": "82aa4242",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T07:26:01.693412Z",
     "start_time": "2025-07-07T07:25:49.629599Z"
    }
   },
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "# ====================== 1. 加载词表 ======================\n",
    "def load_vocab(file_path: str) -> Dict[str, int]:\n",
    "    \"\"\"加载词表映射文件\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# ====================== 2. 定义 Transformer 模型 ======================\n",
    "class PoemTransformer(nn.Module):\n",
    "    \"\"\"Transformer模型（保持原结构，确保兼容性）\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size=256, num_heads=8, num_layers=6, ff_dim=512, dropout=0.1):\n",
    "        super(PoemTransformer, self).__init__()\n",
    "        \n",
    "        # 词向量层\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)  # 主程序中修正padding_idx\n",
    "        \n",
    "        # 位置编码\n",
    "        self.positional_encoding = nn.Parameter(torch.rand(1, 5000, embed_size))\n",
    "        \n",
    "        # 完整Transformer结构\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 输出层\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"生成自回归掩码\"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        # 嵌入和位置编码\n",
    "        src = self.dropout(self.embed(src) + self.positional_encoding[:, :src.size(1), :])\n",
    "        tgt = self.dropout(self.embed(tgt) + self.positional_encoding[:, :tgt.size(1), :])\n",
    "        \n",
    "        # 生成掩码\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        src_mask = self.generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
    "        \n",
    "        # 通过Transformer\n",
    "        output = self.transformer(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        \n",
    "        return self.fc_out(output), None\n",
    "\n",
    "# ====================== 3. 古诗生成器（支持押韵+五言/七言） ======================\n",
    "class PoemGenerator:\n",
    "    def __init__(self,\n",
    "                 token2idx: Dict[str, int],\n",
    "                 idx2token: Dict[int, str],\n",
    "                 model: torch.nn.Module,\n",
    "                 device: str = \"cpu\"):\n",
    "\n",
    "        self.token2idx = token2idx\n",
    "        self.idx2token = idx2token\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.associations = self._load_associations(\"word_associations.json\")\n",
    "        \n",
    "        # 押韵字典（简化版平水韵，按韵母分类）\n",
    "        self.rhyme_dict = self._build_rhyme_dict()\n",
    "        \n",
    "        # 初始化分词器\n",
    "        try:\n",
    "            import thulac\n",
    "            self.thulac = thulac.thulac(seg_only=True)\n",
    "        except ImportError:\n",
    "            print(\"警告: 未安装THULAC分词器，将使用简单分词方法\")\n",
    "            self.thulac = None\n",
    "\n",
    "    def _build_rhyme_dict(self) -> Dict[str, str]:\n",
    "        \"\"\"构建简化版押韵字典（按现代拼音韵母分类）\"\"\"\n",
    "        rhyme_dict = {}\n",
    "        # 基础韵部（覆盖常用字）\n",
    "        rhyme_categories = {\n",
    "            'a': ['花', '家', '霞', '沙', '茶', '麻', '涯', '瓜', '华', '芽', '佳', '斜'],\n",
    "            'o': ['歌', '多', '河', '波', '罗', '柯', '戈', '磨', '蓑', '荷', '婆'],\n",
    "            'i': ['衣', '期', '池', '知', '时', '丝', '诗', '棋', '词', '啼', '溪', '西'],\n",
    "            'u': ['无', '图', '湖', '孤', '壶', '途', '苏', '书', '珠', '浮', '奴'],\n",
    "            'an': ['山', '间', '闲', '还', '颜', '关', '湾', '环', '班', '丹', '残', '天'],\n",
    "            'ang': ['长', '香', '光', '阳', '堂', '芳', '昌', '央', '刚', '桑', '忙'],\n",
    "            'eng': ['风', '声', '灯', '僧', '升', '生', '星', '耕', '更', '情', '城'],\n",
    "            'ong': ['东', '同', '中', '空', '红', '通', '工', '蓬', '浓', '松', '龙']\n",
    "        }\n",
    "        for rhyme, chars in rhyme_categories.items():\n",
    "            for char in chars:\n",
    "                rhyme_dict[char] = rhyme\n",
    "        return rhyme_dict\n",
    "\n",
    "    def _load_associations(self, path: str) -> Dict:\n",
    "        \"\"\"加载关联词库\"\"\"\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        print(f\"警告: 未找到关联词库文件 {path}，将使用空字典。\")\n",
    "        return {}\n",
    "\n",
    "    def segment_with_thulac(self, text: str) -> List[str]:\n",
    "        \"\"\"分词方法\"\"\"\n",
    "        if self.thulac:\n",
    "            result = self.thulac.cut(text)\n",
    "            return [word for word, _ in result]\n",
    "        return list(text)  # 回退到单字分词\n",
    "\n",
    "    def get_associated_words(self, word: str, top_k: int = 5) -> List[str]:\n",
    "        \"\"\"获取输入词的关联词列表\"\"\"\n",
    "        if word not in self.associations:\n",
    "            return []\n",
    "        related_words = sorted(\n",
    "            self.associations[word].items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        return [w for w, _ in related_words[:top_k]]\n",
    "\n",
    "    def generate_poem(self, \n",
    "                     prompt: str, \n",
    "                     style: str = \"五言绝句\",  # 新增：支持\"五言绝句\"或\"七言绝句\"\n",
    "                     max_repeat: int = 2,\n",
    "                     temperature: float = 0.6) -> str:\n",
    "        \"\"\"\n",
    "        生成古诗，支持押韵和句式切换\n",
    "        style: 句式，\"五言绝句\"（5字/句）或\"七言绝句\"（7字/句）\n",
    "        \"\"\"\n",
    "        # 1. 确定句式参数\n",
    "        chars_per_line = 5 if \"五言\" in style else 7\n",
    "        lines_count = 4  # 绝句固定4句\n",
    "        total_chars = chars_per_line * lines_count  # 总字数\n",
    "        \n",
    "        # 2. 分词与联想\n",
    "        segmented_words = self.segment_with_thulac(prompt)\n",
    "        print(f\"分词结果: {segmented_words}\")\n",
    "        \n",
    "        # 收集候选词\n",
    "        all_candidates = []\n",
    "        for word in segmented_words:\n",
    "            all_candidates.append(word)\n",
    "            all_candidates.extend(self.get_associated_words(word, top_k=5))\n",
    "        \n",
    "        # 过滤有效词，补充主题相关常用字\n",
    "        valid_words = list(set(w for w in all_candidates if w in self.token2idx))\n",
    "        if not valid_words:\n",
    "            valid_words = [w for w in list(self.token2idx.keys())[:10] if w not in ['<PAD>', '<START>']]\n",
    "        \n",
    "        # 根据主题补充候选词\n",
    "        theme_related = {\n",
    "            '春天': ['春', '风', '花', '柳', '燕', '啼', '暖', '芽'],\n",
    "            '秋天': ['秋', '霜', '叶', '月', '雁', '寒', '枫', '露'],\n",
    "            '夏天': ['夏', '荷', '蝉', '雨', '荫', '热', '蛙', '莲'],\n",
    "            '冬天': ['冬', '雪', '寒', '梅', '冰', '霜', '风', '松']\n",
    "        }.get(prompt, ['日', '云', '山', '水', '天', '地', '人', '心'])\n",
    "        \n",
    "        valid_words.extend([w for w in theme_related if w in self.token2idx and w not in valid_words])\n",
    "        valid_words = list(set(valid_words))  # 去重\n",
    "        print(f\"有效候选词: {valid_words}\")\n",
    "\n",
    "        # 3. 选择韵部（随机选一个包含候选词的韵部）\n",
    "        candidate_rhymes = set()\n",
    "        for word in valid_words:\n",
    "            if word in self.rhyme_dict:\n",
    "                candidate_rhymes.add(self.rhyme_dict[word])\n",
    "        if not candidate_rhymes:\n",
    "            candidate_rhymes = set(self.rhyme_dict.values())  # 兜底：用所有韵部\n",
    "        target_rhyme = random.choice(list(candidate_rhymes))  # 目标韵部\n",
    "        rhyme_words = [w for w in valid_words if self.rhyme_dict.get(w) == target_rhyme]\n",
    "        if not rhyme_words:\n",
    "            rhyme_words = [w for w in self.rhyme_dict if self.rhyme_dict[w] == target_rhyme]  # 兜底\n",
    "        print(f\"目标韵部: {target_rhyme}, 押韵候选字: {rhyme_words[:5]}\")\n",
    "\n",
    "        # 4. 初始化输入\n",
    "        input_ids = [self.token2idx[valid_words[0]]] if valid_words else [self.token2idx['<START>']]\n",
    "        generated = [self.idx2token[idx] for idx in input_ids]\n",
    "\n",
    "        # 5. 生成诗句（核心逻辑）\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            src = torch.tensor([input_ids]).to(self.device)\n",
    "            tgt = src.clone()\n",
    "            \n",
    "            while len(generated) < total_chars:\n",
    "                # 当前位置信息（第几句、句中第几个字）\n",
    "                current_total = len(generated)\n",
    "                current_line = current_total // chars_per_line  # 0-3\n",
    "                current_pos_in_line = current_total % chars_per_line  # 0到chars_per_line-1\n",
    "                \n",
    "                # 模型预测\n",
    "                output, _ = self.model(src, tgt)\n",
    "                next_probs = output[:, -1, :]  # 最后一个位置的预测\n",
    "                \n",
    "                # 处理重复字符\n",
    "                last_char = generated[-1] if generated else ''\n",
    "                repeat_count = 1\n",
    "                for c in reversed(generated[:-1]):\n",
    "                    if c == last_char:\n",
    "                        repeat_count += 1\n",
    "                    else:\n",
    "                        break\n",
    "                if repeat_count >= max_repeat and last_char in self.token2idx:\n",
    "                    repeat_idx = self.token2idx[last_char]\n",
    "                    next_probs[0, repeat_idx] *= 0.1  # 惩罚重复字符\n",
    "                    next_probs = F.softmax(next_probs, dim=-1)\n",
    "                \n",
    "                # 押韵控制：第二、四句末尾字必须押韵（绝句规则）\n",
    "                need_rhyme = (current_line in [1, 3]) and (current_pos_in_line == chars_per_line - 1)\n",
    "                if need_rhyme:\n",
    "                    # 只保留押韵候选字\n",
    "                    rhyme_indices = [self.token2idx[w] for w in rhyme_words if w in self.token2idx]\n",
    "                    if rhyme_indices:\n",
    "                        # 过滤非押韵字的概率\n",
    "                        mask = torch.ones_like(next_probs)\n",
    "                        for idx in rhyme_indices:\n",
    "                            mask[0, idx] = 0  # 押韵字保留概率\n",
    "                        next_probs = next_probs * (1 - mask)  # 非押韵字概率置0\n",
    "                        next_probs = F.softmax(next_probs, dim=-1)\n",
    "                \n",
    "                # 温度采样\n",
    "                probs = F.softmax(next_probs / temperature, dim=-1)\n",
    "                next_idx = torch.multinomial(probs, 1).item()\n",
    "                next_word = self.idx2token.get(next_idx, '')\n",
    "                \n",
    "                # 过滤无效字符\n",
    "                invalid_tokens = ['<END>', '<UNK>', '<PAD>', '<START>']\n",
    "                if next_word in invalid_tokens:\n",
    "                    continue\n",
    "                \n",
    "                # 重复检查（二次保险）\n",
    "                if generated and next_word == generated[-1] and repeat_count >= max_repeat:\n",
    "                    alternatives = [w for w in valid_words if w != next_word and w not in invalid_tokens]\n",
    "                    if alternatives:\n",
    "                        next_word = random.choice(alternatives)\n",
    "                        next_idx = self.token2idx[next_word]\n",
    "                \n",
    "                # 添加到结果\n",
    "                generated.append(next_word)\n",
    "                input_ids.append(next_idx)\n",
    "                tgt = torch.tensor([input_ids]).to(self.device)\n",
    "        \n",
    "        # 6. 格式化输出（按句式分割）\n",
    "        poem = \"\".join(generated)\n",
    "        for token in invalid_tokens:\n",
    "            poem = poem.replace(token, \"\")\n",
    "        \n",
    "        # 按句分割\n",
    "        lines = []\n",
    "        for i in range(0, min(len(poem), total_chars), chars_per_line):\n",
    "            line = poem[i:i+chars_per_line]\n",
    "            if len(line) == chars_per_line:\n",
    "                lines.append(line)\n",
    "        \n",
    "        # 不足4句则补充\n",
    "        if len(lines) < lines_count and valid_words:\n",
    "            for _ in range(lines_count - len(lines)):\n",
    "                filler = \"\".join(random.sample(valid_words, min(chars_per_line, len(valid_words))))\n",
    "                lines.append(filler[:chars_per_line])\n",
    "        \n",
    "        return \"\\n\".join(lines[:lines_count])\n",
    "\n",
    "# ====================== 4. 模型加载与主程序 ======================\n",
    "def load_model(vocab_size, model_path=None, device='cpu'):\n",
    "    \"\"\"加载模型并处理权重兼容问题\"\"\"\n",
    "    model = PoemTransformer(vocab_size).to(device)\n",
    "    if model_path and os.path.exists(model_path):\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n",
    "            print(f\"成功加载预训练模型: {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"加载模型失败: {e}\")\n",
    "            print(\"将使用随机初始化权重\")\n",
    "    else:\n",
    "        print(\"警告: 未找到预训练模型，将使用随机初始化权重\")\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置设备\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 加载词表\n",
    "    print(\"正在加载词表...\")\n",
    "    token2idx = load_vocab('token2idx.json')\n",
    "    idx2token = {int(k): v for k, v in load_vocab('idx2token.json').items()}\n",
    "    vocab_size = len(token2idx)\n",
    "    print(f\"词表加载完成，大小: {vocab_size}\")\n",
    "    \n",
    "    # 加载模型并修正padding_idx\n",
    "    print(\"正在加载模型...\")\n",
    "    model = load_model(vocab_size, 'poem_transformer.pth', device)\n",
    "    model.embed.padding_idx = token2idx.get('<PAD>', 0)  # 动态修正padding_idx\n",
    "    print(\"模型加载完成\")\n",
    "    \n",
    "    # 初始化生成器\n",
    "    print(\"正在初始化生成器...\")\n",
    "    generator = PoemGenerator(\n",
    "        token2idx=token2idx,\n",
    "        idx2token=idx2token,\n",
    "        model=model,\n",
    "        device=device\n",
    "    )\n",
    "    print(\"生成器初始化完成\")\n",
    "    \n",
    "    # 生成古诗（测试不同句式和主题）\n",
    "    print(\"\\n=== 生成五言绝句（春天主题） ===\")\n",
    "    poem = generator.generate_poem(\"春天\", style=\"五言绝句\", temperature=0.6)\n",
    "    print(poem)\n",
    "    \n",
    "    print(\"\\n=== 生成七言绝句（月亮主题） ===\")\n",
    "    poem = generator.generate_poem(\"月亮\", style=\"七言绝句\", temperature=0.7)\n",
    "    print(poem)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载词表...\n",
      "词表加载完成，大小: 105569\n",
      "正在加载模型...\n",
      "成功加载预训练模型: poem_transformer.pth\n",
      "模型加载完成\n",
      "正在初始化生成器...\n",
      "警告: 未找到关联词库文件 word_associations.json，将使用空字典。\n",
      "Model loaded succeed\n",
      "生成器初始化完成\n",
      "\n",
      "=== 生成五言绝句（春天主题） ===\n",
      "分词结果: ['春天']\n",
      "有效候选词: ['风', '春', '柳', '燕', '暖', '花', '芽', '春天', '啼']\n",
      "目标韵部: eng, 押韵候选字: ['风']\n",
      "风其语俯仰\n",
      "之间白登围\n",
      "振旅有力许\n",
      "由万象森罗\n",
      "\n",
      "=== 生成七言绝句（月亮主题） ===\n",
      "分词结果: ['月亮']\n",
      "有效候选词: ['云', '天', '花藏', '人', '豪富', '日', '心', '遥隔', '<UNK>', '水', '分丛', '地', '以官', '山', '<END>', '上苍']\n",
      "目标韵部: an, 押韵候选字: ['天', '山']\n",
      "云纹生秋向成歌\n",
      "荆插天空寒啸虎\n",
      "苒去除起石饥涎\n",
      "苒一登水塘分别\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ====================== 1. 读取数据 ======================\n",
    "with open('D:/Desktop/tokenized_poems.json', 'r', encoding='utf-8') as f:\n",
    "    poems = json.load(f)  # 直接得到分词后的二维列表\n",
    "\n",
    "# ====================== 2. 构建词表 ======================\n",
    "from collections import Counter\n",
    "\n",
    "# 得到所有token\n",
    "all_tokens_list = [token for poem in poems for para in poem['paragraphs'] for token in para]\n",
    "cnt = Counter(all_tokens_list)\n",
    "print(cnt.most_common(20))  # 查看常见词\n",
    "\n",
    "min_freq = 10  # 只保留出现>=10次的词，其他都归为<UNK>\n",
    "all_tokens = set([token for token in all_tokens_list if cnt[token] >= min_freq])\n",
    "\n",
    "tokens = ['<PAD>', '<START>', '<END>', '<UNK>'] + list(all_tokens)\n",
    "token2idx = {token: idx for idx, token in enumerate(tokens)}\n",
    "idx2token = {idx: token for token, idx in token2idx.items()}\n",
    "vocab_size = len(tokens)\n",
    "print(f\"筛选后词汇表大小: {vocab_size}\")\n",
    "\n"
   ],
   "id": "ccca6f74eb7b9c38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ====================== 3. 数据预处理 ======================\n",
    "\n",
    "lengths = [len(para) for poem in poems for para in poem['paragraphs']]\n",
    "max_content_len = int(np.mean(lengths) + 3)   # 内容部分的最大长度\n",
    "max_len = max_content_len + 2                 # <START> 和 <END>\n",
    "\n",
    "print(f\"每条数据最终长度: {max_len}\")\n",
    "\n",
    "def poem_to_ids(para):\n",
    "    ids = [token2idx['<START>']]\n",
    "    for token in para[:max_content_len]:\n",
    "        if token in token2idx:\n",
    "            ids.append(token2idx[token])\n",
    "        else:\n",
    "            ids.append(token2idx['<UNK>'])\n",
    "    ids.append(token2idx['<END>'])\n",
    "    while len(ids) < max_len:\n",
    "        ids.append(token2idx['<PAD>'])\n",
    "    # 保险起见，如果超长就截断\n",
    "    if len(ids) > max_len:\n",
    "        ids = ids[:max_len]\n",
    "    return ids\n",
    "\n",
    "\n",
    "# 只保留内容长度<=max_content_len的句子\n",
    "data = [poem_to_ids(para) for poem in poems for para in poem['paragraphs'] if len(para) <= max_content_len]\n",
    "\n",
    "# 检查所有样本长度\n",
    "print(\"所有样本长度分布：\", set(len(d) for d in data))\n",
    "\n",
    "\n",
    "# 正确做法应该是遍历所有句子\n",
    "data = [poem_to_ids(para) for poem in poems for para in poem['paragraphs']]\n",
    "for i in range(3):\n",
    "    print(len(data[i]), data[i])\n",
    "\n",
    "print(f\"训练样本数: {len(data)}\")\n",
    "\n",
    "for i in range(3):\n",
    "    print(data[i])\n",
    "    print([idx2token[idx] for idx in data[i]])\n",
    "\n",
    "# ====================== 4. 自定义数据集 ======================\n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][:-1]  # 输入序列\n",
    "        y = self.data[idx][1:]   # 目标序列（预测下一个词）\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "dataset = PoemDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ],
   "id": "c071bf9ac1ab6cf4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ad778e3-56e4-45be-91f2-1927abb2872d",
   "metadata": {},
   "source": [
    "# ====================== 5. 定义RNN模型 ======================\n",
    "class PoemRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=128, hidden_size=256, num_layers=2):\n",
    "        super(PoemRNN, self).__init__()\n",
    "        # 词向量层\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=token2idx['<PAD>'])\n",
    "        # LSTM层\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        # 输出层\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embed(x)  # [B, T, E]\n",
    "        output, hidden = self.lstm(x, hidden)  # output: [B, T, H]\n",
    "        output = self.fc(output)  # [B, T, V]\n",
    "        return output, hidden"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1ffe1985-afe9-43ef-87be-5b7cc56f9a03",
   "metadata": {},
   "source": [
    "# ====================== 6. 训练模型 ======================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = PoemRNN(vocab_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token2idx['<PAD>'])\n",
    "\n",
    "# 训练轮数设置适中，确保训练时间\n",
    "epochs = 15\n",
    "print(f\"词汇表大小: {vocab_size}\")  # 输出词汇表大小\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    # tqdm外层加epoch信息，内层包dataloader显示batch进度\n",
    "    with tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}', ncols=100) as pbar:\n",
    "        for x, y in pbar:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            # 动态显示当前loss\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}')\n",
    "\n",
    "\n",
    "# ====================== 7. 生成古诗函数 ======================\n",
    "def generate_poem(model, start_words=None, max_gen_len=20):\n",
    "    model.eval()\n",
    "    # 初始输入为<START>\n",
    "    input_idx = [token2idx['<START>']]\n",
    "    if start_words:\n",
    "        for word in start_words:\n",
    "            input_idx.append(token2idx.get(word, token2idx['<UNK>']))\n",
    "    input_tensor = torch.tensor([input_idx], dtype=torch.long).to(device)\n",
    "    hidden = None\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_gen_len):\n",
    "            output, hidden = model(input_tensor, hidden)\n",
    "            last_word_logits = output[0, -1]  # 取最后一个时间步\n",
    "            next_word_id = torch.argmax(last_word_logits).item()\n",
    "            next_word = idx2token[next_word_id]\n",
    "            if next_word == '<END>':\n",
    "                break\n",
    "            result.append(next_word)\n",
    "            input_tensor = torch.tensor([[next_word_id]], dtype=torch.long).to(device)\n",
    "    return ''.join(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a5f40cf9-ed3f-4ca0-9c2a-bf2adc7ee39b",
   "metadata": {},
   "source": [
    "# ====================== 8. 示例：生成古诗 ======================\n",
    "print(\"生成古诗示例：\", generate_poem(model, start_words=['湖边', '西', '飞雁']))\n",
    "\n",
    "# ====================== 9. 保存模型 ======================\n",
    "torch.save(model.state_dict(), '../../../张范渝超/qq下载/poem_rnn.pth')\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e56302f-ee9b-423c-b744-0a99c8a9a550",
   "metadata": {},
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "# ====================== 训练结束后保存模型和词典 ======================\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), '../../../张范渝超/qq下载/poem_rnn.pth')\n",
    "\n",
    "# 保存词典\n",
    "token_dict = {\n",
    "    'token2idx': token2idx,\n",
    "    'idx2token': idx2token\n",
    "}\n",
    "\n",
    "with open('token_dict.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(token_dict, f, ensure_ascii=False)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "347be50d-eb96-41e3-8c40-e203497ba8ee",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
